<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Archives: 2014/11 | Alpha&#39;s Blog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Natural Language Processing | Mechine Learning | Pattern Recognition | Language Model | Deep Learning | Recurrent Neutral Network | Speech Recognition">
<meta property="og:type" content="website">
<meta property="og:title" content="Alpha's Blog">
<meta property="og:url" content="http://alpha19881007.com/archives/2014/11/">
<meta property="og:site_name" content="Alpha's Blog">
<meta property="og:description" content="Natural Language Processing | Mechine Learning | Pattern Recognition | Language Model | Deep Learning | Recurrent Neutral Network | Speech Recognition">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Alpha's Blog">
<meta name="twitter:description" content="Natural Language Processing | Mechine Learning | Pattern Recognition | Language Model | Deep Learning | Recurrent Neutral Network | Speech Recognition">

  
    <link rel="alternative" href="/atom.xml" title="Alpha&#39;s Blog" type="application/atom+xml">
  
  
    <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico">
  
  <link href="//fonts.googleapis.com/css?family=Inconsolata:400,700|Open+Sans:700,400" rel="stylesheet" type="text/css">
  <link rel="stylesheet" href="/css/style.css" type="text/css">

  

</head>
<body>
  <div id="container">
    <div id="wrap">
      <div id="header">
  <div id="header-outer" class="outer">
    <div id="header-inner" class="inner">
      <div id="header-title">
        <h1 id="logo-wrap">
          <a href="/" id="logo">Alpha&#39;s Blog
          
              <span id="subtitle">Try to Understand, Try to Conquer</span>
          
          </a>
        </h1>
      </div>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" results="0" class="search-form-input" placeholder="Search"><input type="submit" value="&#xF002;" class="search-form-submit"><input type="hidden" name="q" value="site:http://alpha19881007.com"></form>
      </div>
    </div>
  </div>
</div>
      <div class="outer">
        <section id="main">
  
    <article id="post-BP-Neural-NetWork-on-Hadoop" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2014/11/09/BP-Neural-NetWork-on-Hadoop/" class="article-date">
  <time datetime="2014-11-09T09:08:21.000Z" itemprop="datePublished">11月 9 2014</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Machine-Learning/">Machine Learning</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2014/11/09/BP-Neural-NetWork-on-Hadoop/">BP Neural NetWork on Hadoop</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="Problem_Source">Problem Source</h2>
<p>In June this year, I get started with hadoop and always wonder its meaning to me. After reading a book named Data Intensive Text Processing with Mapreduce, I gradually know what hadoop can do and what can not. </p>
<p>Hadoop suits batch processing tasks well, but no interaction among sub-tasks limits its usage. For example, top down or bottom up based word clustering can hardly be handled by hadoop. Great efforts have been made to push hadoop into industrial usage. As we see, <a href="http://mahout.apache.org/" target="_blank" rel="external">mahout</a> is an open source project founded by Apache, which contains implementation of several machine learning algorithms, such as k-means, naive bayes, recommendation system, etc. If you are interested, download from <a href="http://svn.apache.org/repos/asf/mahout/trunk" target="_blank" rel="external">here</a>.</p>
<p>Can hadoop train a BP neural network or not? This question troubles me a lot. You probably see stocastic gradient descent, batch gradient descent and gradient descent are often the ways to optimize weight matrixes in neural network trainning process.</p>
<p>Batch and stocastic based gradient descent methods all need to update weight matrix during processing samples. So there must be a global share state to keep newest model parameters, and current model state should be read before every feed forward step. Fine-grained synchronization will have significant impact on parallization, what applies to hadoop.</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://alpha19881007.com/2014/11/09/BP-Neural-NetWork-on-Hadoop/" data-id="ei4yvg63kzfg99ih" class="article-share-link">Share</a>
      
        <a href="http://alpha19881007.com/2014/11/09/BP-Neural-NetWork-on-Hadoop/#disqus_thread" class="article-comment-link">Comments</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Hadoop/">Hadoop</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Neural-Network/">Neural Network</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-New-Word-Extraction" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2014/11/07/New-Word-Extraction/" class="article-date">
  <time datetime="2014-11-07T09:35:08.000Z" itemprop="datePublished">11月 7 2014</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Natural-Language-Processing/">Natural Language Processing</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2014/11/07/New-Word-Extraction/">New Word Extraction</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="Introduction">Introduction</h2>
<p>As we all know, vocabulary plays an important role in speech recognition, machine translation, natural language understanding and other NLP jobs. So efficiently automatic new word extraction from large scale corpus can release manual burden dramatically. In my experience, most e-commerce companies emphasize new word collection, because those new words will be regarded as features and do help in their subsequent analytic jobs.</p>
<p>The main idea of this blog comes from one article from web, which you can refer to <a href="http://www.matrix67.com/blog/archives/5044" target="_blank" rel="external">here</a>. After reading, I find the idea proposed is so cool and amazing, so I decide to implement it. By coincidence, I have just buit our language model trainning framework on hadoop platform at that time, and hadoop become my choice naturally. This is the first time I get a taste of the charm of data mining. Thanks to <a href="http://www.matrix67.com/" target="_blank" rel="external">Matrix67</a>.</p>
<h2 id="Main_Idea">Main Idea</h2>
<p>Before we step into details, We should think over a question first, namely, how a word can be a real word? This question is quite weird, isn’t it? But different answers to it generate totally different ways of processing. In Matrix67’s article, he mainly mantains two points. First, those words before and behind this candidate word should be as more diverse as possible. Second, the inner coherence of this word also needs to be as strong as possible.</p>
<p>As for the first point above, that is to say we hope few words can cooperate with the candidate word to make a fixed combination. And it is natural to use <a href="http://zh.wikipedia.org/wiki/%E7%86%B5" target="_blank" rel="external">entropy</a> to measure the diversity around a candidate word. For the second point, the candidate word is probably made by several sub words in return, so we do hope the probability of this word higher than those sub words appear independently in corpus. This time we design a simple strategy to decide the similarity of word distribution.</p>
<p>May be here you still can not believe that such simple idea can leads to admiring results, but it does.</p>
<h2 id="Details">Details</h2>
<h3 id="Word_Around_Diversity">Word Around Diversity</h3>
<p>It’s not difficult to compute entropy around a candidate word thinking in mapreduce. Here suppose you’ve already known some basic knowledge about <a href="http://hadoop.apache.org" target="_blank" rel="external">hadoop</a>, like hdfs, mapreduce and so on. To compute entropy around a candidate word, we need to take several steps as follow.</p>
<p>First, split corpus lines into a large amount of candidate words using suffix representation.</p>
<p>Second, find out all words around a candidate word and count their occurence one by one, then compute its left and right entropy by (1) respectively. No doult, this step should go through all candidate words.</p>
<p>$$E_w=-\sum_{w}{p(w)logp(w)}(1)$$</p>
<p>Last, entropy threshold needs to be set, in my experiment, it’s 1.5. If the left and right entropy of a candidate word are both greater than this threshold value, then this word will be passed to next stage filtering.</p>
<h3 id="Word_Inner_Coherence">Word Inner Coherence</h3>
<p>After above steps, we’ve got relative qualified candidate words, but we still need to filter them in a finer way. Now we are ready to take word inner coherence into our consideration. In fact, it is not easy to do so because of memory limit. Here we first take following steps to compute word inner coherence.</p>
<p>First, compute word frequency when we have got the result from suffix representation step.</p>
<p>Second, computed by a simple formula down below, in which $w_i$ and $w_j$ is just one way to form $w$, we choose the maximal one as the index to that word coherence.</p>
<p>$$S_w=\max\log\frac{p(w)}{p(w_i)*p(w_j)}(2)$$</p>
<p>Equation (2) is similar to the one used in <a href="http://word2vec.googlecode.com/svn/trunk/" target="_blank" rel="external">vec2phrase</a>, which is illustrated in equation (3). The differences here are we probably have more than one combination to form $w$, and use probability instead.</p>
<p>$$S_w=\frac{cnt(w)-alpha}{cnt(w_i)*cnt(w_j)}(3)$$</p>
<p>Last, again we need a threshold value to filter word coherence, this time in my experiment,  word inner coherence greater than 6.5 will be regarded as a qulified one.</p>
<h3 id="Roadblocks">Roadblocks</h3>
<p>Till now, have you ever asked a question to yourself? If your current corpus size is more than 50G, what the corresponding size of output from above first step will be? Ha, it must be several times over 100G. It’s huge, isn’t it?  Even worse, its scale goes up according to your corpus size exponentially, so it can not be loaded to memory directly. Unfortunately above second computation step needs word frequency information, so this is actually the problem we have to figure out.</p>
<p>Trie tree is the first idea coming to my mind which can dramatically reduce memory usage and have a broad usage in NLP domain. But quikly I find this way is not practical, because those candidate words are not real words so trie tree can not do a big help. And also in the working mechanism of hadoop, this data structure could not be shared by all sub-tasks, that means in every task we do need to load those data into memory. </p>
<h3 id="Solution">Solution</h3>
<p>I have tried for several days to find out a way, but never make it until one day after having a nap I start to wonder may be <a href="http://zh.wikipedia.org/wiki/%E5%80%92%E6%8E%92%E7%B4%A2%E5%BC%95" target="_blank" rel="external">inverted index</a>, which play significant role in web searching engine, can give us some inspirations. In my work, if we want to do some computations about one candidate word, we only need to gather imforation, which can be recorded into one line in corpus, related to it and no more. Let’s think it and make it in mapreduce way! </p>
<p>I choose to do my experiment on chinese corpus, so here my example is a chinese sentence, such as “我爱北京天安门”. You can see the problem can be solved by two mapreduce programs as I illustrate below.</p>
<figure class="highlight"><figcaption><span>[program 1]</span></figcaption><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div></pre></td><td class="code"><pre><div class="line"><span class="tag">Map</span> <span class="tag">stage</span> </div><div class="line">我                                  <span class="attr_selector">[w:我爱北京天安门]</span></div><div class="line">我                                  <span class="attr_selector">[p:0.001]</span></div><div class="line">爱北京天安门                        <span class="attr_selector">[w:我爱北京天安门]</span></div><div class="line">爱北京天安门                        <span class="attr_selector">[p:0.01]</span></div><div class="line">我爱                                <span class="attr_selector">[w:我爱北京天安门]</span></div><div class="line">我爱                                <span class="attr_selector">[p:0.01]</span></div><div class="line">北京天安门                          <span class="attr_selector">[w:我爱北京天安门]</span></div><div class="line">北京天安门                          <span class="attr_selector">[p:0.02]</span></div><div class="line">我爱北                              <span class="attr_selector">[w:我爱北京天安门]</span></div><div class="line">我爱北                              <span class="attr_selector">[p:0.1]</span></div><div class="line">京天安门                            <span class="attr_selector">[w:我爱北京天安门]</span></div><div class="line">京天安门                            <span class="attr_selector">[p:0.1]</span></div><div class="line">我爱北京                            <span class="attr_selector">[w:我爱北京天安门]</span></div><div class="line">我爱北京                            <span class="attr_selector">[p:0.03]</span></div><div class="line">天安门                              <span class="attr_selector">[w:我爱北京天安门]</span></div><div class="line">天安门                              <span class="attr_selector">[p:0.2]</span></div><div class="line">我爱北京天                          <span class="attr_selector">[w:我爱北京天安门]</span></div><div class="line">我爱北京天                          <span class="attr_selector">[p:0.02]</span></div><div class="line">安门                                <span class="attr_selector">[w:我爱北京天安门]</span></div><div class="line">安门                                <span class="attr_selector">[p:0.01]</span></div><div class="line">我爱北京天安                        <span class="attr_selector">[w:我爱北京天安门]</span></div><div class="line">我爱北京天安                        <span class="attr_selector">[p:0.01]</span></div><div class="line">门                                  <span class="attr_selector">[w:我爱北京天安门]</span></div><div class="line">门                                  <span class="attr_selector">[p:0.01]</span></div><div class="line">我爱北京天安门                      <span class="attr_selector">[w:我爱北京天安门]</span></div><div class="line">我爱北京天安门                      <span class="attr_selector">[p:0.1]</span></div><div class="line"></div><div class="line"><span class="tag">Reduce</span> <span class="tag">stage</span></div><div class="line">我                                  <span class="attr_selector">[w:我爱北京天安门, p:0.001]</span></div><div class="line">爱北京天安门                        <span class="attr_selector">[w:我爱北京天安门, p:0.01]</span></div><div class="line">我爱                                <span class="attr_selector">[w:我爱北京天安门, p:0.01]</span></div><div class="line">北京天安门                          <span class="attr_selector">[w:我爱北京天安门, p:0.02]</span></div><div class="line">我爱北                              <span class="attr_selector">[w:我爱北京天安门, p:0.1]</span></div><div class="line">京天安门                            <span class="attr_selector">[w:我爱北京天安门, p:0.1]</span></div><div class="line">我爱北京                            <span class="attr_selector">[w:我爱北京天安门, p:0.03]</span></div><div class="line">天安门                              <span class="attr_selector">[w:我爱北京天安门, p:0.2]</span></div><div class="line">我爱北京天                          <span class="attr_selector">[w:我爱北京天安门, p:0.02]</span></div><div class="line">安门                                <span class="attr_selector">[w:我爱北京天安门, p:0.01]</span></div><div class="line">我爱北京天安                        <span class="attr_selector">[w:我爱北京天安门, p:0.01]</span></div><div class="line">门                                  <span class="attr_selector">[w:我爱北京天安门, p:0.01]</span></div><div class="line">我爱北京天安门                      <span class="attr_selector">[w:我爱北京天安门, p:0.1]</span></div></pre></td></tr></table></figure><br>The function of this mapreduce program mainly list sub-words of a candidate word and combine its frequency imformation. And those candidate words have same sub-words will be merged in the reduce stage which are not showed here by the help of hadoop mechanism. So in next mapreduce program, we can get a candidate word and its sub-words information naturally.<br><br><figure class="highlight"><figcaption><span>[program 2]</span></figcaption><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="tag">Map</span> <span class="tag">stage</span> </div><div class="line">我爱北京天安门                      <span class="attr_selector">[我:0.001]</span></div><div class="line">我爱北京天安门                      <span class="attr_selector">[爱北京天安门:0.01]</span></div><div class="line">我爱北京天安门                      <span class="attr_selector">[我爱:0.01]</span></div><div class="line">我爱北京天安门                      <span class="attr_selector">[北京天安门:0.02]</span></div><div class="line">我爱北京天安门                      <span class="attr_selector">[我爱北:0.1]</span></div><div class="line">我爱北京天安门                      <span class="attr_selector">[京天安门:0.1]</span></div><div class="line">我爱北京天安门                      <span class="attr_selector">[我爱北京:0.03]</span></div><div class="line">我爱北京天安门                      <span class="attr_selector">[天安门:0.2]</span></div><div class="line">我爱北京天安门                      <span class="attr_selector">[我爱北京天:0.02]</span></div><div class="line">我爱北京天安门                      <span class="attr_selector">[安门:0.01]</span></div><div class="line">我爱北京天安门                      <span class="attr_selector">[我爱北京天安:0.01]</span></div><div class="line">我爱北京天安门                      <span class="attr_selector">[门:0.01]</span></div><div class="line">我爱北京天安门                      <span class="attr_selector">[我爱北京天安门:0.1]</span></div><div class="line"></div><div class="line"><span class="tag">Reduce</span> <span class="tag">stage</span></div><div class="line">我爱北京天安门                      <span class="attr_selector">[我:0.001，爱北京天安门:0.01，我爱:0.01，北京天安门:0.02，我爱北:0.1，京天安门:0.1，我爱北京:0.03，天安门:0.2，我爱北京天:0.02，安门:0.01，我爱北京天安:0.01，门:0.01，我爱北京天安门:0.1]</span></div></pre></td></tr></table></figure>

<p>All probabilities showed here are not real statistical values from corpus. When we get to this point, it’s easy for us to compute the inner coherence of all those candidate words, and filter them by the threshold set in advance. The process is very easy, isn’t it? Finally I have my own implementation of this idea on hadoop, So you can do it too. Here are part words extracted from 35G chinese corpus.</p>
<p><figure class="highlight"><figcaption><span>[part result]</span></figcaption><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">蚂蚁 疙瘩 蘑菇 蜘蛛 捕鱼达人 崩溃 脂肪 喇叭 忐忑 葡萄 槟榔 琵琶 钥匙 咳嗽 矛盾 螃蟹 猥琐    </div><div class="line">枯藤老树昏鸦 遗憾 蝴蝶 遥远的她 枸杞 瑜伽 郁闷 藏头诗 尺寸 引擎 咖啡 锻炼 胶囊 恐怖 糊涂 嫖娼 </div><div class="line">垃圾 玻璃 寂寞 癌症 呜呜 细胞 详细 芙蓉 盱眙龙虾 狐狸 欺负 讨厌 拒绝 郭德纲的相声 郭德刚的相声</div><div class="line">雁南飞 幽默 狙击 舅舅 痔疮 睾丸 麦当劳 左右 潇洒 郭德纲 违章 扁担 迷茫 猎豹 赤壁 倒霉 邯郸 玫瑰</div><div class="line">监狱 恢复 推荐 愤怒的小鸟 萝卜 纽约 奔驰 研究 秘诀 培训 监督 媳妇 选择 HELLO 谦虚 GOOGLE</div></pre></td></tr></table></figure><br>HIGH    舞蹈    轩辕剑  兼职    驾驶    菠萝    必须    矜持    兮兮    娱乐场所  麻辣烫  疲劳    披萨    KTV    玫瑰花的葬礼    嫖妓</p>
<h2 id="End">End</h2>
<p>Although words extracted from above steps are very dramastic, there are still some fake words pollute our result. But we have ways to handle this problem to some extent, such as rule based means. Now we can compare our result with common dictionary and extract new words, and also compare the result of this month corpus with the one aquired last month. That is to say if we consider different dimensions like time and corpus type, we can get different interesting results. As presented in Matrix’s article, we can do a lot as long as we use our imagination. Thank you!</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://alpha19881007.com/2014/11/07/New-Word-Extraction/" data-id="tmfy8nrl6ema566w" class="article-share-link">Share</a>
      
        <a href="http://alpha19881007.com/2014/11/07/New-Word-Extraction/#disqus_thread" class="article-comment-link">Comments</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/New-Word-Extraction/">New Word Extraction</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Word-Segment/">Word Segment</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-About" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2014/11/07/About/" class="article-date">
  <time datetime="2014-11-07T05:42:38.000Z" itemprop="datePublished">11月 7 2014</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Diary/">Diary</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2014/11/07/About/">About Me</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>This blog is owned by Alpha. Alpha graduated from Nanjing University of Science &amp; Technology in April 2014. Research direction in postgraduate period is related to pattern recognition and machine learning.</p>
<p>His current job correlates to ASR (acoustical speech recognition) and LM (language model). To be honest, I always think he is a lucky dog, because his job gets a lot to do with its original research direction. He has much interest in natural language processing, data minning and also distributed computation (hadoop, spark and storm), and I do see he works hard to follow closely the steps of latest trend in those domains.</p>
<p>No pains, no gains. Ha, no matter what do you think, He does belive that anyway. The reason why He decides to write blog in English is trying to keep and promote writing ability which easily degradate by time.</p>
<p>He will record his work and study here, and wish all of you guys to come and comment, if you have any advice and problem. And here is email address <a href="">xuwenqiang0563@qq.com</a>. Let you learn together and work together!</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://alpha19881007.com/2014/11/07/About/" data-id="t56ks9sznr7iyhf9" class="article-share-link">Share</a>
      
        <a href="http://alpha19881007.com/2014/11/07/About/#disqus_thread" class="article-comment-link">Comments</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-My-First-Post" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2014/11/07/My-First-Post/" class="article-date">
  <time datetime="2014-11-07T02:03:20.000Z" itemprop="datePublished">11月 7 2014</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Diary/">Diary</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2014/11/07/My-First-Post/">My First Hexo Journey</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="Welcome_to_My_Hexo_Blog_Zone!">Welcome to My Hexo Blog Zone!</h2>
<p>So excited to share life things with you guys here. To celebrate my first virgin journey, I would like to write down some words below.</p>
<p>I always wish to find one cool place to record the little drops of my life, but never succeed until several days before when I suddenly met hexo developed by a student in Taiwan. Hexo is a kind of static web page generator, and by which we can build our own personal blog on our local machine, then move it over to github. Unlike common blog systems, web pages generated by hexo is very clean and tidy, there is no ad and other annoying things, and also the blog theme can be customized. So cool and I really love it, thanks a lot to the author. If you want to know more about hexo, you can click <a href="http://hexo.io" target="_blank" rel="external">here</a>.</p>
<p>Ha, Let’s start our hexo blog journey now. But first we need to get familiar to hexo grammar.</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://alpha19881007.com/2014/11/07/My-First-Post/" data-id="xusxg179p05v6nf1" class="article-share-link">Share</a>
      
        <a href="http://alpha19881007.com/2014/11/07/My-First-Post/#disqus_thread" class="article-comment-link">Comments</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Hexo-Journey/">Hexo Journey</a></li></ul>

    </footer>
  </div>
  
</article>


  
  
</section>
        
          
            <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Diary/">Diary</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Machine-Learning/">Machine Learning</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Natural-Language-Processing/">Natural Language Processing</a><span class="category-list-count">1</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recents</h3>
    <div class="widget recents">
      <ul>
        
          <li>
            <a href="/2014/11/09/BP-Neural-NetWork-on-Hadoop/">BP Neural NetWork on Hadoop</a>
          </li>
        
          <li>
            <a href="/2014/11/07/New-Word-Extraction/">New Word Extraction</a>
          </li>
        
          <li>
            <a href="/2014/11/07/About/">About Me</a>
          </li>
        
          <li>
            <a href="/2014/11/07/My-First-Post/">My First Hexo Journey</a>
          </li>
        
      </ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/Hadoop/" style="font-size: NaNpx;">Hadoop</a><a href="/tags/Hexo-Journey/" style="font-size: NaNpx;">Hexo Journey</a><a href="/tags/Neural-Network/" style="font-size: NaNpx;">Neural Network</a><a href="/tags/New-Word-Extraction/" style="font-size: NaNpx;">New Word Extraction</a><a href="/tags/Word-Segment/" style="font-size: NaNpx;">Word Segment</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2014/11/">November 2014</a><span class="archive-list-count">4</span></li></ul>
    </div>
  </div>

  
</aside>
          
        
      </div>
      <div id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      <a rel="license" href="http://creativecommons.org/licenses/by-nc/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-nc/4.0/88x31.png" /></a></br>
      &copy; 2014 Wenqiang.Xu<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</div>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    
<script>
  var disqus_shortname = 'alpha19881007';
  
  (function(){
    var dsq = document.createElement('script');
    dsq.type = 'text/javascript';
    dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/count.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
  })();
</script>


<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css" type="text/css">

  <script src="/fancybox/jquery.fancybox.pack.js" type="text/javascript"></script>



<script src="/js/script.js" type="text/javascript"></script>

x config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


  </div>
</body>
</html>