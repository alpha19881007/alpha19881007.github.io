<!DOCTYPE HTML>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Natural Language Processing | Alpha&#39;s Blog</title>
  <meta name="author" content="Wenqiang.Xu">
  
  <meta name="description" content="Natural Language Processing | Mechine Learning | Pattern Recognition | Language Model | Deep Learning | Recurrent Neutral Network | Speech Recognition">
  
  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  
  <meta property="og:site_name" content="Alpha&#39;s Blog"/>

  
    <meta property="og:image" content="undefined"/>
  

  <link href="/favicon.png" rel="icon">
  <link rel="alternate" href="/atom.xml" title="Alpha&#39;s Blog" type="application/atom+xml">
  <link rel="stylesheet" href="/css/style.css" media="screen" type="text/css">
  <!--[if lt IE 9]><script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]-->
  <script src="//ajax.googleapis.com/ajax/libs/jquery/1.8/jquery.min.js"></script>
  
</head>

<body>
  <header id="header" class="inner"><div class="alignleft">
  <h1><a href="/">Alpha&#39;s Blog</a></h1>
  <h2><a href="/">Try to Understand, Try to Conquer</a></h2>
</div>
<nav id="main-nav">
  <ul>
    
      <li><a href="/">Home</a></li>
    
      <li><a href="/archives">Archives</a></li>
    
      <li><a href="/about">About</a></li>
    
  </ul>
  <div class="clearfix"></div>
</nav>
<div class="clearfix"></div>
<div class="alignleft" style="margin-top: 15px">


</div>
<div class="clearfix"></div>
</header>
  <div id="content" class="inner">
    <div id="wrapper">
<h2 class="archive-title category">Natural Language Processing</h2>


  
    <article class="post">
  
    <div class="gallery">
  <div class="photoset">
    
      <img src="">
    
  </div>
  <div class="control">
    <div class="prev"></div>
    <div class="next"></div>
  </div>
</div>
  
  <div class="post-content">
    <header>
      
  
    <h1 class="title"><a href="/2014/11/07/New-Word-Extraction/">New Word Extraction</a></h1>
  

      
        <p class="published">
          Published: <time datetime="2014-11-07T09:35:08.000Z">11月 7 2014</time>
        </p>
      
    </header>
    <div class="entry">
      
        <h2 id="Introduction">Introduction</h2>
<p>As we all know, vocabulary plays an important role in speech recognition, machine translation, natural language understanding and other NLP jobs. So efficiently automatic new word extraction from large scale corpus can release manual burden dramatically. In my experience, most e-commerce company emphasize new word collection, because those new words will be regarded as features and do help in their subsequent analytic jobs.</p>
<p>The main idea of this blog comes from one article from web, which you can refer to <a href="http://www.matrix67.com/blog/archives/5044" target="_blank" rel="external">here</a>. After reading, I find the idea proposed is so cool and amazing, so I decide to implement it. By coincidence I have just buit our language model trainning framework on hadoop platform at that time, and hadoop become my choice naturally. This is the first time I get a taste of the charm of data minning. Thanks to Matrix67.</p>
<h2 id="Main_Idea">Main Idea</h2>
<p>Befor we step into details, We should think over a question first, namely, how a word can be a real word? This question is quite weird, isn’t it? But different answers to it generate totally different way of processing. In Matrix67’s article, he mainly mantains two points. First, those words before and behind this candidate word should be as more diverse as possible. Second, the inner coherence of this word also need to be as strong as possible.</p>
<p>As for the first point above, that is to say we hope few words can cooperate with the candidate word to make a fixed combination. And it is natural to use entropy to measure the diversity around a candidate word. For the second point, the candidate word is probably made by several sub words in return, so we do hope the probablity of this word higher than those sub words appear independently in corpus. This time we can use a simple ralative entropy form to decide the similarity of word distribution.</p>
<p>May be here you still can not believe that such simple idea can leads to admiring results, but it does.</p>
<h2 id="Roadblocks">Roadblocks</h2>
<h3 id="Word_Around_Diversity">Word Around Diversity</h3>
<p>It’s not difficult to compute entropy around a candidate word thinking in mapreduce. Here suppose you’v already known some basic knowledge about <a href="http://hadoop.apache.org" target="_blank" rel="external">hadoop</a>, like hdfs, mapreduce and so on. To compute entropy around a candidate word, we need to have several steps as follow.</p>
<p>First, split corpus lines into a large amount of candidate words using suffix representation; </p>
<p>Second, find out all words around a candidate word and count their occurence each, then compute its left and right entropy respectively. No doult, this step should go through all candidate words;</p>
<p>Last, entropy threshold need to be set, in my experiment, it’s 1.5. If the left and right entropy of a candidate word all greater than this threshold value, then this word will be passed to next stage filtering.</p>
<h3 id="Word_Inner_Coherence">Word Inner Coherence</h3>
<p>After above steps, we’v got real candidate words, but we still need to filter them in more fine way. Now we are ready to take word inner coherence into our consideration. In fact, it is not easy to do so, because of memory limit. Here we first give following steps to compute word inner coherence.</p>
<p>First, compute word frequency when we have got the result from suffix representation step;</p>
<p>Second, by using simple KL form, we can get the distance between a candidate word and its sub words combination respectively, and then choose maximal one as index to word coherence.</p>
<p>Last, again we need to set a threshold value to filter word coherence, this time in my experiment,  word inner coherence greater than 6.5 will be qulified word.</p>
<p>Till now, have you ever asked a question to yourself? If your current corpus size is more than 50G, what the corresponding size of output from above first step will be? Ha, it must be several times over 100G. It’s huge, isn’t it?  Even worse, its scale goes up according to your corpus size exponentially, so can not be loaded to memory directly. Unfortunately above second computation step need word frequency information, so this is the real problem we have to figure out.</p>

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>
<!-- hackish -->

  

  <nav id="pagination">
  
  
  <div class="clearfix"></div>
</nav>
</div>
    <div class="clearfix"></div>
  </div>
  <footer id="footer" class="inner"><div class="alignleft">
  
  &copy; 2014 Wenqiang.Xu
  
</div>
<div class="clearfix"></div></footer>
  <script src="/js/jquery.imagesloaded.min.js"></script>
<script src="/js/gallery.js"></script>




<link rel="stylesheet" href="/fancybox/jquery.fancybox.css" media="screen" type="text/css">
<script src="/fancybox/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
(function($){
  $('.fancybox').fancybox();
})(jQuery);
</script>

</body>
</html>