<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title><![CDATA[Alpha's Blog]]></title>
  <subtitle><![CDATA[Try to Understand, Try to Conquer]]></subtitle>
  <link href="/atom.xml" rel="self"/>
  <link href="http://alpha19881007.com/"/>
  <updated>2014-11-20T03:39:16.263Z</updated>
  <id>http://alpha19881007.com/</id>
  
  <author>
    <name><![CDATA[Wenqiang.Xu]]></name>
    <email><![CDATA[xuwenqiang0563@gmail.com]]></email>
  </author>
  
  <generator uri="http://zespia.tw/hexo/">Hexo</generator>
  
  <entry>
    <title><![CDATA[BP Neural NetWork on Hadoop]]></title>
    <link href="http://alpha19881007.com/2014/11/09/BP-Neural-NetWork-on-Hadoop/"/>
    <id>http://alpha19881007.com/2014/11/09/BP-Neural-NetWork-on-Hadoop/</id>
    <published>2014-11-09T09:08:21.000Z</published>
    <updated>2014-11-20T03:02:42.000Z</updated>
    <content type="html"><![CDATA[<h2 id="Problem_Source">Problem Source</h2>
<p>In June this year, I start to approach hadoop and always wonder its meaning to me. After reading a book named Data Intensive Text Processing with Mapreduce, I gradually know what hadoop can do and what can not. </p>
<p>Hadoop suits batch processing tasks well, but no interaction among sub tasks limits its usage. For example, top down or bottom up based word clustering can not be handled by hadoop. Great efforts have been made to push hadoop into industrial usage. As we see, <a href="http://mahout.apache.org/" target="_blank" rel="external">mahout</a> is an open source project founded by Apache, which contains implementation of several machine learning algorithms, such as k-means, naive bayes, recommendation system etc. If you have interest, download from <a href="http://svn.apache.org/repos/asf/mahout/trunk" target="_blank" rel="external">here</a>.</p>
<p>Whether hadoop can train a BP neural network or not, this question troubles me a lot. You probably see stocastic gradient descent, batch gradient descent and gradient descent are often the ways to optmize weight matrixes in neural network trainning process.</p>
]]></content>
    <summary type="html">
    <![CDATA[<h2 id="Problem_Source">Problem Source</h2>
<p>In June this year, I start to approach hadoop and always wonder its meaning to me. After read]]>
    </summary>
    
      <category term="Hadoop" scheme="http://alpha19881007.com/tags/Hadoop/"/>
    
      <category term="Neural Network" scheme="http://alpha19881007.com/tags/Neural-Network/"/>
    
      <category term="Machine Learning" scheme="http://alpha19881007.com/categories/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[New Word Extraction]]></title>
    <link href="http://alpha19881007.com/2014/11/07/New-Word-Extraction/"/>
    <id>http://alpha19881007.com/2014/11/07/New-Word-Extraction/</id>
    <published>2014-11-07T09:35:08.000Z</published>
    <updated>2014-11-20T03:34:40.000Z</updated>
    <content type="html"><![CDATA[<h2 id="Introduction">Introduction</h2>
<p>As we all know, vocabulary plays an important role in speech recognition, machine translation, natural language understanding and other NLP jobs. So efficiently automatic new word extraction from large scale corpus can release manual burden dramatically. In my experience, most e-commerce company emphasize new word collection, because those new words will be regarded as features and do help in their subsequent analytic jobs.</p>
<p>The main idea of this blog comes from one article from web, which you can refer to <a href="http://www.matrix67.com/blog/archives/5044" target="_blank" rel="external">here</a>. After reading, I find the idea proposed is so cool and amazing, so I decide to implement it. By coincidence I have just buit our language model trainning framework on hadoop platform at that time, and hadoop become my choice naturally. This is the first time I get a taste of the charm of data minning. Thanks to <a href="http://www.matrix67.com/" target="_blank" rel="external">Matrix67</a>.</p>
<h2 id="Main_Idea">Main Idea</h2>
<p>Befor we step into details, We should think over a question first, namely, how a word can be a real word? This question is quite weird, isn’t it? But different answers to it generate totally different way of processing. In Matrix67’s article, he mainly mantains two points. First, those words before and behind this candidate word should be as more diverse as possible. Second, the inner coherence of this word also need to be as strong as possible.</p>
<p>As for the first point above, that is to say we hope few words can cooperate with the candidate word to make a fixed combination. And it is natural to use <a href="http://zh.wikipedia.org/wiki/%E7%86%B5" target="_blank" rel="external">entropy</a> to measure the diversity around a candidate word. For the second point, the candidate word is probably made by several sub words in return, so we do hope the probablity of this word higher than those sub words appear independently in corpus. This time we design a simple strategy to decide the similarity of word distribution.</p>
<p>May be here you still can not believe that such simple idea can leads to admiring results, but it does.</p>
<h2 id="Roadblocks">Roadblocks</h2>
<h3 id="Word_Around_Diversity">Word Around Diversity</h3>
<p>It’s not difficult to compute entropy around a candidate word thinking in mapreduce. Here suppose you’v already known some basic knowledge about <a href="http://hadoop.apache.org" target="_blank" rel="external">hadoop</a>, like hdfs, mapreduce and so on. To compute entropy around a candidate word, we need to have several steps as follow.</p>
<p>First, split corpus lines into a large amount of candidate words using suffix representation.</p>
<p>Second, find out all words around a candidate word and count their occurence each, then compute its left and right entropy respectively. No doult, this step should go through all candidate words.</p>
<p>Last, entropy threshold need to be set, in my experiment, it’s 1.5. If the left and right entropy of a candidate word all greater than this threshold value, then this word will be passed to next stage filtering.</p>
<h3 id="Word_Inner_Coherence">Word Inner Coherence</h3>
<p>After above steps, we’v got relative qualified candidate words, but we still need to filter them in more fine way. Now we are ready to take word inner coherence into our consideration. In fact, it is not easy to do so, because of memory limit. Here we first give following steps to compute word inner coherence.</p>
<p>First, compute word frequency when we have got the result from suffix representation step.</p>
<p>Second, computed by a simple formula down below, in which $w_i$ and $w_j$ is just one way to form $w$, we choose maximal one as the index to that word coherence.</p>
<p>\begin{aligned}<br>Sw{_i}{_j}=\log \frac{p(w)}{p(w_i)*p(w_j)}(1) \\<br>Sw = max Sw{_i}{_j}(2)<br>\end{aligned}</p>
<p>Equation (1) is similar to the one used in <a href="http://word2vec.googlecode.com/svn/trunk/" target="_blank" rel="external">vec2phrase</a>, which is illustrated in equation (3). The differences here are we probably have more than one combination to form $w$, and use probability instead.</p>
<p>$$Sw=\frac{cnt(w_i,w_j)-alpha}{cnt(w_i)*cnt(w_j)}(3)$$</p>
<p>Last, again we need a threshold value to filter word coherence, this time in my experiment,  word inner coherence greater than 6.5 will be regarded as a qulified one.</p>
<p>Till now, have you ever asked a question to yourself? If your current corpus size is more than 50G, what the corresponding size of output from above first step will be? Ha, it must be several times over 100G. It’s huge, isn’t it?  Even worse, its scale goes up according to your corpus size exponentially, so can not be loaded to memory directly. Unfortunately above second computation step need word frequency information, so this is the real problem we have to figure out.</p>
]]></content>
    <summary type="html">
    <![CDATA[<h2 id="Introduction">Introduction</h2>
<p>As we all know, vocabulary plays an important role in speech recognition, machine translation, na]]>
    </summary>
    
      <category term="New Word Extraction" scheme="http://alpha19881007.com/tags/New-Word-Extraction/"/>
    
      <category term="Word Segment" scheme="http://alpha19881007.com/tags/Word-Segment/"/>
    
      <category term="Natural Language Processing" scheme="http://alpha19881007.com/categories/Natural-Language-Processing/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[My First Hexo Journey]]></title>
    <link href="http://alpha19881007.com/2014/11/07/My-First-Post/"/>
    <id>http://alpha19881007.com/2014/11/07/My-First-Post/</id>
    <published>2014-11-07T02:03:20.000Z</published>
    <updated>2014-11-09T12:48:59.000Z</updated>
    <content type="html"><![CDATA[<h2 id="Welcome_to_My_Hexo_Blog_Zone!">Welcome to My Hexo Blog Zone!</h2>
<p>So excited to share life things with you guys here. To celebrate my first virgin journey, I would like to write down some words below.</p>
<p>I always wish to find one cool place to record the little drops of my life, but never succeed until several days before when I suddenly met hexo developed by a student in Taiwan. Hexo is a kind of static web page generator, and by which we can build our own personal blog on our local machine, then move it over to github. Unlike common blog systems, web pages generated by hexo is very clean and tidy, no ad and other annoying things, and also the blog theme can be customized. So cool and I really love it, thanks a lot to the author. If you want to know more about hexo, you can click <a href="http://hexo.io" target="_blank" rel="external">here</a>.</p>
<p>Ha, Let’s start our hexo blog journey now. But first we need get familiar to hexo grammar.</p>
]]></content>
    <summary type="html">
    <![CDATA[<h2 id="Welcome_to_My_Hexo_Blog_Zone!">Welcome to My Hexo Blog Zone!</h2>
<p>So excited to share life things with you guys here. To celebrat]]>
    </summary>
    
      <category term="Hexo Journey" scheme="http://alpha19881007.com/tags/Hexo-Journey/"/>
    
      <category term="Diary" scheme="http://alpha19881007.com/categories/Diary/"/>
    
  </entry>
  
</feed>
