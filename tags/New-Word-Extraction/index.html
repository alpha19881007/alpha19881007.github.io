<!DOCTYPE HTML>
<html>
<head>
  <meta charset="utf-8">
  
  <title>New Word Extraction | Alpha&#39;s Blog</title>
  <meta name="author" content="Wenqiang.Xu">
  
  <meta name="description" content="Natural Language Processing | Mechine Learning | Pattern Recognition | Language Model | Deep Learning | Recurrent Neutral Network | Speech Recognition">
  
  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  
  <meta property="og:site_name" content="Alpha&#39;s Blog"/>

  
    <meta property="og:image" content="undefined"/>
  

  <link href="/favicon.png" rel="icon">
  <link rel="alternate" href="/atom.xml" title="Alpha&#39;s Blog" type="application/atom+xml">
  <link rel="stylesheet" href="/css/style.css" media="screen" type="text/css">
  <!--[if lt IE 9]><script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]-->
  <script src="//ajax.googleapis.com/ajax/libs/jquery/1.8/jquery.min.js"></script>
  
</head>

<body>
  <header id="header" class="inner"><div class="alignleft">
  <h1><a href="/">Alpha&#39;s Blog</a></h1>
  <h2><a href="/">Try to Understand, Try to Conquer</a></h2>
</div>
<nav id="main-nav">
  <ul>
    
      <li><a href="/">Home</a></li>
    
      <li><a href="/archives">Archives</a></li>
    
      <li><a href="/about">About</a></li>
    
  </ul>
  <div class="clearfix"></div>
</nav>
<div class="clearfix"></div>
<div class="alignleft" style="margin-top: 15px">


</div>
<div class="clearfix"></div>
</header>
  <div id="content" class="inner">
    <div id="wrapper">
<h2 class="archive-title tag">New Word Extraction</h2>


  
    <article class="post">
  
    <div class="gallery">
  <div class="photoset">
    
      <img src="">
    
  </div>
  <div class="control">
    <div class="prev"></div>
    <div class="next"></div>
  </div>
</div>
  
  <div class="post-content">
    <header>
      
  
    <h1 class="title"><a href="/2014/11/07/New-Word-Extraction/">New Word Extraction</a></h1>
  

      
        <p class="published">
          Published: <time datetime="2014-11-07T09:35:08.000Z">11月 7 2014</time>
        </p>
      
    </header>
    <div class="entry">
      
        <h2 id="Introduction">Introduction</h2>
<p>As we all know, vocabulary plays an important role in speech recognition, machine translation, natural language understanding and other NLP jobs. So efficiently automatic new word extraction from large scale corpus can release manual burden dramatically. In my experience, most e-commerce companies emphasize new word collection, because those new words will be regarded as features and do help in their subsequent analytic jobs.</p>
<p>The main idea of this blog comes from one article from web, which you can refer to <a href="http://www.matrix67.com/blog/archives/5044" target="_blank" rel="external">here</a>. After reading, I find the idea proposed is so cool and amazing, so I decide to implement it. By coincidence, I have just buit our language model trainning framework on hadoop platform at that time, and hadoop become my choice naturally. This is the first time I get a taste of the charm of data mining. Thanks to <a href="http://www.matrix67.com/" target="_blank" rel="external">Matrix67</a>.</p>
<h2 id="Main_Idea">Main Idea</h2>
<p>Before we step into details, We should think over a question first, namely, how a word can be a real word? This question is quite weird, isn’t it? But different answers to it generate totally different ways of processing. In Matrix67’s article, he mainly mantains two points. First, those words before and behind this candidate word should be as more diverse as possible. Second, the inner coherence of this word also needs to be as strong as possible.</p>
<p>As for the first point above, that is to say we hope few words can cooperate with the candidate word to make a fixed combination. And it is natural to use <a href="http://zh.wikipedia.org/wiki/%E7%86%B5" target="_blank" rel="external">entropy</a> to measure the diversity around a candidate word. For the second point, the candidate word is probably made by several sub words in return, so we do hope the probability of this word higher than those sub words appear independently in corpus. This time we design a simple strategy to decide the similarity of word distribution.</p>
<p>May be here you still can not believe that such simple idea can leads to admiring results, but it does.</p>
<h2 id="Details">Details</h2>
<h3 id="Word_Around_Diversity">Word Around Diversity</h3>
<p>It’s not difficult to compute entropy around a candidate word thinking in mapreduce. Here suppose you’ve already known some basic knowledge about <a href="http://hadoop.apache.org" target="_blank" rel="external">hadoop</a>, like hdfs, mapreduce and so on. To compute entropy around a candidate word, we need to take several steps as follow.</p>
<p>First, split corpus lines into a large amount of candidate words using suffix representation.</p>
<p>Second, find out all words around a candidate word and count their occurence one by one, then compute its left and right entropy by (1) respectively. No doult, this step should go through all candidate words.</p>
<p>$$E_w=-\sum_{w}{p(w)logp(w)}(1)$$</p>
<p>Last, entropy threshold needs to be set, in my experiment, it’s 1.5. If the left and right entropy of a candidate word are both greater than this threshold value, then this word will be passed to next stage filtering.</p>
<h3 id="Word_Inner_Coherence">Word Inner Coherence</h3>
<p>After above steps, we’ve got relative qualified candidate words, but we still need to filter them in a finer way. Now we are ready to take word inner coherence into our consideration. In fact, it is not easy to do so because of memory limit. Here we first take following steps to compute word inner coherence.</p>
<p>First, compute word frequency when we have got the result from suffix representation step.</p>
<p>Second, computed by a simple formula down below, in which $w_i$ and $w_j$ is just one way to form $w$, we choose the maximal one as the index to that word coherence.</p>
<p>$$S_w=\max\log\frac{p(w)}{p(w_i)*p(w_j)}(2)$$</p>
<p>Equation (2) is similar to the one used in <a href="http://word2vec.googlecode.com/svn/trunk/" target="_blank" rel="external">vec2phrase</a>, which is illustrated in equation (3). The differences here are we probably have more than one combination to form $w$, and use probability instead.</p>
<p>$$S_w=\frac{cnt(w)-alpha}{cnt(w_i)*cnt(w_j)}(3)$$</p>
<p>Last, again we need a threshold value to filter word coherence, this time in my experiment,  word inner coherence greater than 6.5 will be regarded as a qulified one.</p>
<h3 id="Roadblocks">Roadblocks</h3>
<p>Till now, have you ever asked a question to yourself? If your current corpus size is more than 50G, what the corresponding size of output from above first step will be? Ha, it must be several times over 100G. It’s huge, isn’t it?  Even worse, its scale goes up according to your corpus size exponentially, so it can not be loaded to memory directly. Unfortunately above second computation step needs word frequency information, so this is actually the problem we have to figure out.</p>
<p>Trie tree is the first idea coming to my mind which can dramatically reduce memory usage and have a broad usage in NLP domain. But quikly I find this way is not practical, because those candidate words are not real words so trie tree can not do a big help. And also in the working mechanism of hadoop, this data structure could not be shared by all sub-tasks, that means in every task we do need to load those data into memory. </p>
<h3 id="Solution">Solution</h3>
<p>I have tried for several days to find out a way, but never make it until one day after having a nap I start to wonder may be <a href="http://zh.wikipedia.org/wiki/%E5%80%92%E6%8E%92%E7%B4%A2%E5%BC%95" target="_blank" rel="external">inverted index</a>, which play significant role in web searching engine, can give us some inspirations. In my work, if we want to do some computations about one candidate word, we only need to gather imforation, which can be recorded into one line in corpus, related to it and no more. Let’s think it and make it in mapreduce way! </p>
<p>I choose to do my experiment on chinese corpus, so here my example is a chinese sentence, such as “我爱北京天安门”. You can see the problem can be solved by two mapreduce programs as I illustrate below.</p>
<p>Map stage<br>我======[w:我爱北京天安门]<br>我======[p:0.001]<br>爱北京天安门======[w:我爱北京天安门]<br>爱北京天安门======[p:0.01]<br>我爱======[w:我爱北京天安门]<br>我爱======[p:0.01]<br>北京天安门======[w:我爱北京天安门]<br>北京天安门======[p:0.02]<br>我爱北======[w:我爱北京天安门]<br>我爱北======[p:0.1]<br>京天安门======[w:我爱北京天安门]<br>京天安门======[p:0.1]<br>我爱北京======[w:我爱北京天安门]<br>我爱北京======[p:0.03]<br>天安门======w:我爱北京天安门<br>天安门======[p:0.2]<br>我爱北京天======[w:我爱北京天安门]<br>我爱北京天======[p:0.02]<br>安门======[w:我爱北京天安门]<br>安门======[p:0.01]<br>我爱北京天安======[w:我爱北京天安门]<br>我爱北京天安======[p:0.01]<br>门======[w:我爱北京天安门]<br>门======[p:0.01]<br>我爱北京天安门======[w:我爱北京天安门]<br>我爱北京天安门======[p:0.1]</p>
<p>Reduce stage<br>我======[w:我爱北京天安门, p:0.001]<br>爱北京天安门======[w:我爱北京天安门, p:0.01]<br>我爱======[w:我爱北京天安门, p:0.01]<br>北京天安门======[w:我爱北京天安门, p:0.02]<br>我爱北======[w:我爱北京天安门, p:0.1]<br>京天安门======[w:我爱北京天安门, p:0.1]<br>我爱北京======[w:我爱北京天安门, p:0.03]<br>天安门======[w:我爱北京天安门, p:0.2]<br>我爱北京天======[w:我爱北京天安门, p:0.02]<br>安门======[w:我爱北京天安门, p:0.01]<br>我爱北京天安======[w:我爱北京天安门, p:0.01]<br>门======[w:我爱北京天安门, p:0.01]<br>我爱北京天安门======[w:我爱北京天安门, p:0.1]</p>
<p>The function of this mapreduce program mainly list sub-words of a candidate word and combine its frequency imformation. And those candidate words have same sub-words will be merged in the reduce stage which are not showed here by the help of hadoop mechanism. So in next mapreduce program, we can get a candidate word and its sub-words information naturally.</p>
<p>Map stage<br>我爱北京天安门======[我:0.001]<br>我爱北京天安门======[爱北京天安门:0.01]<br>我爱北京天安门======[我爱:0.01]<br>我爱北京天安门======[北京天安门:0.02]<br>我爱北京天安门======[我爱北:0.1]<br>我爱北京天安门======[京天安门:0.1]<br>我爱北京天安门======[我爱北京:0.03]<br>我爱北京天安门======[天安门:0.2]<br>我爱北京天安门======[我爱北京天:0.02]<br>我爱北京天安门======[安门:0.01]<br>我爱北京天安门======[我爱北京天安:0.01]<br>我爱北京天安门======[门:0.01]<br>我爱北京天安门======[我爱北京天安门:0.1]</p>
<p>Reduce stage<br>我爱北京天安门======[我:0.001，爱北京天安门:0.01，我爱:0.01，北京天安门:0.02，我爱北:0.1，京天安门:0.1，我爱北京:0.03，天安门:0.2，我爱北京天:0.02，安门:0.01，我爱北京天安:0.01，门:0.01，我爱北京天安门:0.1]</p>
<p>All probabilities showed here are not real statistical values from corpus. When we get to this point, it’s easy for us to compute the inner coherence of all those candidate words, and filter them by the threshold set in advance. The process is very easy, isn’t it? Finally I have my own implementation of this idea on hadoop, So you can do it too. Here are part words extracted from 35G chinese corpus.</p>
<p>蚂蚁    疙瘩     蘑菇    蜘蛛    捕鱼达人 崩溃    脂肪    喇叭     忐忑    葡萄    槟榔     琵琶    钥匙    咳嗽     矛盾    螃蟹    猥琐<br>枯藤老树昏鸦    遗憾    蝴蝶     遥远的她   枸杞    瑜伽     郁闷    藏头诗  尺寸     引擎    咖啡    锻炼     胶囊    恐怖    糊涂  嫖娼<br>垃圾    玻璃     寂寞    癌症    呜呜     细胞    详细    芙蓉     盱眙龙虾     狐狸    欺负     讨厌    拒绝    郭德纲的相声  郭德刚的相声<br>雁南飞  幽默    狙击    舅舅    痔疮    睾丸    麦当劳  左右    潇洒    郭德纲  违章    扁担    迷茫    猎豹    赤壁    倒霉    邯郸    玫瑰<br>监狱    恢复    推荐    愤怒的小鸟 萝卜    纽约    奔驰    研究    秘诀    培训    监督    媳妇    选择    HELLO   谦虚    GOOGLE<br>HIGH    舞蹈    轩辕剑  兼职    驾驶    菠萝    必须    矜持    兮兮    娱乐场所  麻辣烫  疲劳    披萨    KTV    玫瑰花的葬礼    嫖妓</p>
<h2 id="End">End</h2>
<p>Although words extracted from above steps are very dramastic, there are still some fake words pollute our result. But we have ways to handle this problem to some extent, such as rule based means. Now we can compare our result with common dictionary and extract new words, and also compare the result of this month corpus with the one aquired last month. That is to say if we consider different dimensions like time and corpus type, we can get different interesting results. As presented in Matrix’s article, we can do a lot as long as we use our imagination. Thank you!</p>

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>
<!-- hackish -->

  

  <nav id="pagination">
  
  
  <div class="clearfix"></div>
</nav>
</div>
    <div class="clearfix"></div>
  </div>
  <footer id="footer" class="inner"><div class="alignleft">
  
  &copy; 2014 Wenqiang.Xu
  
</div>
<div class="clearfix"></div></footer>
  <script src="/js/jquery.imagesloaded.min.js"></script>
<script src="/js/gallery.js"></script>


<script type="text/javascript">
var disqus_shortname = 'alpha19881007';

(function(){
  var dsq = document.createElement('script');
  dsq.type = 'text/javascript';
  dsq.async = true;
  dsq.src = '//' + disqus_shortname + '.disqus.com/count.js';
  (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
}());
</script>



<link rel="stylesheet" href="/fancybox/jquery.fancybox.css" media="screen" type="text/css">
<script src="/fancybox/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
(function($){
  $('.fancybox').fancybox();
})(jQuery);
</script>
x config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>



</body>
</html>
